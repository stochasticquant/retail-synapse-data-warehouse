{
	"name": "Retail_Landing_to_Bronze_Audit",
	"properties": {
		"folder": {
			"name": "Retail_Company"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e572acbf-e314-42ca-b62f-2e356f9e069c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/55b82f65-c34b-4db6-ba28-24d60af6cb30/resourceGroups/synapsedata-warehouse-rg/providers/Microsoft.Synapse/workspaces/musa-synapse/bigDataPools/sparkpool",
				"name": "sparkpool",
				"type": "Spark",
				"endpoint": "https://musa-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Retail: Landing (CSV) → Bronze (Parquet) with Audit & Completeness (Synapse Spark)\n",
					"\n",
					"**Scope:** Retail only. Reads landing CSVs from `abfss://data@<account>/retail_company/`, writes Parquet to\n",
					"`abfss://bronze@<account>/retail_company/<Table>/ingestion_date=YYYY-MM-DD/`, quarantines bad rows, and writes a JSON\n",
					"manifest to `abfss://bronze@<account>/audit/retail_company/run=<UTC_ISO>/manifest.json`.\n",
					"\n",
					"**Checks implemented:** row counts, file/byte counts, quarantine count + sampling, schema snapshot,\n",
					"column count match, optional header validation, basic profiling (nulls + date min/max), duplicate file detection,\n",
					"cross-run idempotency, late-arriving flag (optional), encoding & delimiter sniff.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from datetime import datetime, timezone\n",
					"\n",
					"\n",
					"ACCOUNT = \"musasourcedata\"\n",
					"USECASE = \"retail_company\"\n",
					"LANDING = f\"abfss://data@{ACCOUNT}.dfs.core.windows.net\"\n",
					"BRONZE  = f\"abfss://bronze@{ACCOUNT}.dfs.core.windows.net\"\n",
					"INGEST_DATE  = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
					"RUN_TS_UTC   = datetime.utcnow().replace(tzinfo=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
					"MANIFEST_DIR = f\"{BRONZE}/audit/{USECASE}/run={RUN_TS_UTC}\"\n",
					"QUAR_DIR     = f\"{BRONZE}/_quarantine/{USECASE}\"\n",
					"CTRL_DIR     = f\"{BRONZE}/_control/{USECASE}\"\n",
					"TABLES = [\"Customers\",\"Employees\",\"Orders\",\"Products\",\"OrderItems\",\"Payments\",\"Shipments\",\"Returns\",\"Warehouses\",\"InventorySnapshot\"]\n",
					"HEADER_SPEC = {}\n",
					"LATE_ARRIVING_SPEC = {}\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F, types as T\n",
					"import json, hashlib, re\n",
					"try:\n",
					"    mfs = mssparkutils.fs\n",
					"except NameError:\n",
					"    mfs = None\n",
					"def fs_list(path):\n",
					"    if mfs is not None:\n",
					"        try:\n",
					"            return mfs.ls(path)\n",
					"        except Exception:\n",
					"            return []\n",
					"    return []\n",
					"def fs_head(path, n=4096):\n",
					"    if mfs is not None:\n",
					"        try:\n",
					"            return mfs.head(path, n)\n",
					"        except Exception:\n",
					"            return \"\"\n",
					"    return \"\"\n",
					"def fs_put(path, payload):\n",
					"    if mfs is not None:\n",
					"        mfs.put(path, payload, True)\n",
					"        return True\n",
					"    return False\n",
					"def sniff_encoding_and_delimiter(sample: str):\n",
					"    enc = \"utf-8\"\n",
					"    if \"\\x00\" in sample:\n",
					"        enc = \"utf-16\"\n",
					"    comma_count = sample.count(\",\")\n",
					"    tab_count   = sample.count(\"\\t\")\n",
					"    delim = \",\" if comma_count >= tab_count else \"\\t\"\n",
					"    return enc, delim\n",
					"def file_fingerprint(path_obj):\n",
					"    p = path_obj.path\n",
					"    sz = int(getattr(path_obj, \"size\", 0) or 0)\n",
					"    lm = int(getattr(path_obj, \"modificationTime\", 0) or 0)\n",
					"    head = fs_head(p, 1024)\n",
					"    head_hash = hashlib.sha256(head.encode(\"utf-8\",\"ignore\")).hexdigest()[:16]\n",
					"    return f\"{p}|{sz}|{lm}|{head_hash}\"\n",
					"def read_csv_permissive(csv_path, delimiter=\",\"):\n",
					"    return (spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"mode\",\"PERMISSIVE\").option(\"multiLine\",\"true\").option(\"quote\",\"\\\"\").option(\"escape\",\"\\\"\").option(\"delimiter\", delimiter).csv(csv_path))\n",
					"def get_schema_snapshot(df):\n",
					"    return [ (f.name, str(f.dataType), f.nullable) for f in df.schema.fields ]\n",
					"def basic_profile(df):\n",
					"    cols = df.columns\n",
					"    total = df.count()\n",
					"    if total == 0:\n",
					"        return {\"row_count\": 0, \"pct_nulls_per_column\": {c: 0.0 for c in cols}, \"date_minmax\": {}}\n",
					"    nulls = [F.sum(F.when(F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), 1).otherwise(0)).alias(c) for c in cols]\n",
					"    null_counts = df.select(nulls).collect()[0].asDict()\n",
					"    pct_nulls = {c: (null_counts[c] / total) for c in cols}\n",
					"    date_cols = [c for c in cols if re.search(r\"date\", c, flags=re.IGNORECASE)]\n",
					"    date_minmax = {}\n",
					"    for c in date_cols:\n",
					"        d = F.coalesce(F.to_date(F.col(c), \"yyyy-MM-dd\"), F.to_date(F.col(c), \"dd/MM/yyyy\"), F.to_date(F.col(c), \"MM-dd-yyyy\"), F.to_date(F.col(c), \"yyyy/MM/dd\"))\n",
					"        mm = df.select(F.min(d).alias(\"min\"), F.max(d).alias(\"max\")).collect()[0]\n",
					"        date_minmax[c] = {\"min\": str(mm[\"min\"]) if mm[\"min\"] else None, \"max\": str(mm[\"max\"]) if mm[\"max\"] else None}\n",
					"    return {\"row_count\": total, \"pct_nulls_per_column\": pct_nulls, \"date_minmax\": date_minmax}\n",
					"def write_json_manifest(obj, target_dir, filename=\"manifest.json\"):\n",
					"    path = f\"{target_dir}/{filename}\"\n",
					"    fs_put(path, json.dumps(obj, indent=2))\n",
					"    return path\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Load cross-run processed fingerprints (idempotency)\n",
					"PROCESSED_PATH = f\"{CTRL_DIR}/processed_files.json\"\n",
					"processed_fingerprints = set()\n",
					"try:\n",
					"    txt = fs_head(PROCESSED_PATH, 5000000)\n",
					"    if txt:\n",
					"        processed_fingerprints = set(json.loads(txt))\n",
					"except Exception:\n",
					"    processed_fingerprints = set()\n",
					"print(\"Loaded processed fingerprints:\", len(processed_fingerprints))\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Run manifest and processing\n",
					"landing_base = f\"{LANDING}/{USECASE}\"\n",
					"run_manifest = {\"usecase\": USECASE, \"run_utc\": RUN_TS_UTC, \"ingestion_date\": INGEST_DATE, \"landing_base\": landing_base, \"bronze_base\": f\"{BRONZE}/{USECASE}\",\n",
					"                \"checks\": {\"row_counts\": True, \"file_counts_bytes\": True, \"quarantine_count\": True, \"schema_snapshot\": True, \"column_count_match\": True,\n",
					"                           \"header_validation\": True, \"basic_profile\": True, \"duplicate_file_detection\": True, \"idempotency\": True, \"late_arriving_flag\": True,\n",
					"                           \"encoding_delimiter_sniff\": True, \"bad_row_sampling\": True},\n",
					"                \"tables\": []}\n",
					"\n",
					"for tbl in TABLES:\n",
					"    rec = {\"table\": tbl, \"status\": \"PENDING\", \"landing\": {}, \"bronze\": {}, \"quarantine\": {}, \"validation\": {}, \"profile\": {}, \"notes\": []}\n",
					"    csv_path = f\"{landing_base}/{tbl}.csv\"\n",
					"    files = fs_list(f\"{LANDING}/{USECASE}/\")\n",
					"    fmatch = [f for f in files if f.path.endswith(f\"/{tbl}.csv\")]\n",
					"    if not fmatch:\n",
					"        rec[\"status\"] = \"FAIL\"; rec[\"notes\"].append(\"Landing CSV not found.\"); run_manifest[\"tables\"].append(rec); continue\n",
					"    finfo = fmatch[0]\n",
					"    head_sample = fs_head(csv_path, 4096)\n",
					"    enc, delim = sniff_encoding_and_delimiter(head_sample)\n",
					"    rec[\"landing\"][\"encoding_guess\"] = enc; rec[\"landing\"][\"delimiter_guess\"] = \"\\\\t\" if delim == \"\\t\" else delim\n",
					"    fp = file_fingerprint(finfo)\n",
					"    rec[\"landing\"][\"fingerprint\"] = fp\n",
					"    if fp in processed_fingerprints:\n",
					"        rec[\"status\"] = \"SKIPPED\"; rec[\"notes\"].append(\"Duplicate fingerprint from a previous run; skipped.\"); run_manifest[\"tables\"].append(rec); continue\n",
					"    rec[\"landing\"][\"file_path\"] = finfo.path; rec[\"landing\"][\"file_size\"] = int(getattr(finfo,\"size\",0) or 0); rec[\"landing\"][\"file_count\"] = 1\n",
					"    df_raw = read_csv_permissive(csv_path, delimiter=delim)\n",
					"    rec[\"landing\"][\"header\"] = df_raw.columns; rec[\"landing\"][\"column_count\"] = len(df_raw.columns)\n",
					"    if tbl in HEADER_SPEC and HEADER_SPEC[tbl]:\n",
					"        exp = HEADER_SPEC[tbl]\n",
					"        rec[\"validation\"][\"header_expected\"] = exp\n",
					"        rec[\"validation\"][\"header_matches\"]  = (exp == df_raw.columns)\n",
					"        if not rec[\"validation\"][\"header_matches\"]: rec[\"notes\"].append(\"Header mismatch vs spec.\")\n",
					"    corrupt_col = \"_corrupt_record\"; has_corrupt = corrupt_col in df_raw.columns\n",
					"    if has_corrupt:\n",
					"        good_df = df_raw.filter(F.col(corrupt_col).isNull()).drop(corrupt_col)\n",
					"        bad_df  = df_raw.filter(F.col(corrupt_col).isNotNull())\n",
					"    else:\n",
					"        good_df = df_raw; bad_df = spark.createDataFrame([], df_raw.schema)\n",
					"    landing_count = df_raw.count(); rec[\"landing\"][\"row_count\"] = landing_count\n",
					"    bronze_dir = f\"{BRONZE}/{USECASE}/{tbl}/ingestion_date={INGEST_DATE}\"\n",
					"    good_df.coalesce(1).write.mode(\"append\").parquet(bronze_dir)\n",
					"    bad_cnt = bad_df.count(); rec[\"quarantine\"][\"bad_rows\"] = bad_cnt\n",
					"    if bad_cnt>0:\n",
					"        qdir = f\"{QUAR_DIR}/{tbl}/ingestion_date={INGEST_DATE}\"; bad_df.write.mode(\"append\").json(qdir); bad_df.limit(100).write.mode(\"overwrite\").json(f\"{qdir}/sample\")\n",
					"    bronze_df = spark.read.parquet(bronze_dir); bronze_count = bronze_df.count(); rec[\"bronze\"][\"row_count\"] = bronze_count\n",
					"    try:\n",
					"        bfiles = fs_list(bronze_dir); rec[\"bronze\"][\"file_count\"] = len([x for x in bfiles if not x.isDir]); rec[\"bronze\"][\"total_bytes\"] = int(sum(int(getattr(x,\"size\",0) or 0) for x in bfiles))\n",
					"    except Exception:\n",
					"        rec[\"bronze\"][\"file_count\"] = None; rec[\"bronze\"][\"total_bytes\"] = None\n",
					"    rec[\"validation\"][\"column_count_match\"] = (len(df_raw.columns) == len(bronze_df.columns))\n",
					"    rec[\"bronze\"][\"schema_snapshot\"] = get_schema_snapshot(bronze_df)\n",
					"    rec[\"profile\"] = basic_profile(bronze_df)\n",
					"    if tbl in LATE_ARRIVING_SPEC:\n",
					"        col = LATE_ARRIVING_SPEC[tbl][\"column\"]; watermark = LATE_ARRIVING_SPEC[tbl][\"min\"]\n",
					"        dcol = F.coalesce(F.to_date(F.col(col), \"yyyy-MM-dd\"), F.to_date(F.col(col), \"dd/MM/yyyy\"), F.to_date(F.col(col), \"MM-dd-yyyy\"), F.to_date(F.col(col), \"yyyy/MM/dd\"))\n",
					"        min_dt = bronze_df.select(F.min(dcol).alias(\"min\")).collect()[0][\"min\"]\n",
					"        rec[\"validation\"][\"late_arriving_flag\"] = bool(min_dt and str(min_dt) < watermark)\n",
					"        if rec[\"validation\"][\"late_arriving_flag\"]: rec[\"notes\"].append(f\"Late-arriving rows: min {min_dt} < watermark {watermark}\")\n",
					"    else:\n",
					"        rec[\"validation\"][\"late_arriving_flag\"] = None\n",
					"    rec[\"status\"] = \"OK\" if landing_count == bronze_count else \"WARN\"\n",
					"    if rec[\"status\"] == \"WARN\": rec[\"notes\"].append(f\"RowCount mismatch landing={landing_count}, bronze={bronze_count}\")\n",
					"    processed_fingerprints.add(fp)\n",
					"    run_manifest[\"tables\"].append(rec)\n",
					"\n",
					"manifest_path = write_json_manifest(run_manifest, MANIFEST_DIR, filename=\"manifest.json\")\n",
					"print(\"Wrote manifest:\", manifest_path)\n",
					"fs_put(f\"{CTRL_DIR}/processed_files.json\", json.dumps(list(processed_fingerprints), indent=2))\n",
					"print(\"Updated:\", f\"{CTRL_DIR}/processed_files.json\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Next steps\n",
					"- Remove `coalesce(1)` for larger data and add a periodic compaction job.\n",
					"- Add strict `HEADER_SPEC` per table and optional schema contracts in `_control/retail_company/schemas`.\n",
					"- Trigger from a Synapse Pipeline (Get Metadata → Notebook → Success/Failure guard via manifest)."
				]
			}
		]
	}
}